[{"title":"SimPoE Simulated Character Control for 3d human pose estimation","url":"/2021/10/01/SimPoE%20Simulated%20Character%20Control%20for%203d%20human%20pose%20estimation/","content":"todo:\n\n了解为什么没有 flaws，然后具体是怎么解决的\n了解一下 root torque 是怎么分成 15 steps 来 apply 的，看看之前的 paper\naction generation unit 不是 RL 吗，但是这里好像是 MLP 直接 infer action？？？\n增加 metric 的 formula 啥的\n\n1. intro1.1 problem:kinematics 已经很多研究了，但是 dynamics 还没怎么研究.\n\nkinematic: 主要集中在 geometric relationships of 3D poses and 2D images. 但是只考虑 positions 的，文章指出因为没考虑 physical forces (dynamics), 所以经常出现物理条件下不可能的情况：\n\n穿模\n在视频中预测的 3d 模型抖动\n脚在地面上滑行\n\n\ndynamics: 也关注了 physical forces 等类似 friction，joint torque 啥的\n\n\n1.2 加入了 dynamics 的 related works:大部分都是 先 kinematic motion estimation, 之后再用 physics-based trajectory optimization (two stages)\n但是存在问题：\n\ntrajectory optimization highly complex，而且不是 train 出来的，只是 post-processing 的一个 step，所以 test-time 有很长 latency\n用 simple，differentiable physics model 来 optimization，但是更准确的 model 都是 un-differentiable 的，所以这种办法可能有 high optimization error.\n因为是 post-processing, 不能 end2end learning\n\n1.3 用 RL 来模拟 character control 的\n有 manually driven awards 的\n有 GAIL based method RL 不用 reward engineering 的\n\n来达到 long-term behavior，有的 work 用了 hierarchical RL 来 control characters\n最近的 work 来学习 user-controllable policies from motion capture data. 但是，这种学习智能 reproduce training motions, 对于没见过的就不行了\n\n????? 了解一下具体是怎么有 flaws，然后这个是怎么解决的\n\n2. method\n2.1 在 simulation 里面创建一个人的 model用 SMPL model 的算法 类似 VIBE 来产生 skinned mesh human model，这些模型提供了：\n\nskeleton of  bones\nmesh of  vertices\nskinning weight matrix , 每个 i-j element 都代表了 influence of  th-bone’s transformation on  th-vertex’s position\n接着把每个 vertex 分配到 bone，从  里面看哪个 bone 对 这个 vertex 的影响最大就分配给 哪个bone， 计算得到  \n用 convex hull 来计算 bone 的 geometry，接着假设 constant density of bone，mass 就由 geometry 决定\n\n\n\n2.2 simulate character control2.2.1 用 PPO 算法（2017）来做 RL2.2.2 定义 states：$s_t = (q_t, \\dot{q_t}, \\tilde{q}{t+1}, \\check{x}{t+1}, c_{t+1})$:\n 是 current pose\n 是 joint velocities\n 是 estimated kinetic pose\n 是 2D keypoints\n 是 keypoint confidence\n\n2.2.3 定义 actions:\n一般的 action 设计是计算 每个 non-root joint 的 从一帧到下一帧的 torque. \n\n一个 sampling difference 问题：这里存在 video 是 30Hz 的，但是 physics simulator 是 450Hz 的，所以我们每帧之间的 action 需要分配成 15 个 simulation steps. 这里我们使用 proportional-derivative(PD) controllers 来分解这 15 个 steps，生成 15 个action来得到最终效果，PD controller 类似于 damped spring. \n\nPD 的 定义： \n\n 就是每一个 step 施加的 torque\n 是 target joint angles of PD controllers\n 分别代表 在这个 step 开始的时候的 joint angles 和 joint angle velocity\n 分别代表 stiffness 和 dampness of spring, 是 hyperparam，需要 manual 调整\n\n\n我们之后 介绍 meta-PD controller, 为了弥补上面需要 手动调整  的问题，我们引入 \n\nprior works 介绍了如果给 root joint 加 torque 提升模型 robustness，所以 action 再 predict residual forces and torque  给 root\n\n？？？？？？这里文章没说 root torque 是怎么分成 15 steps 的，如果没有用 meta-PD 的话，就可能 apply same torque 15 times 来训练也行，可能隐藏在 prior works 里面了，参考 “residual force control for agile human behavior immitation and extended motion synthesis. 2020”\n\n\n\n\n\n2.2.4 定义 rewards：\n\n\n 是 pose reward, 计算 difference of local joint orientations  and ground truth \n\n\n\n\n is number of joints,  is relative rotation between two rotations\n\n\n is velocity reward\n is 3d world joint position reward\n\n\n\n\n is 2d projection of joints to match 2d joints\n\n\n这里选择 multiplication of sub-awards 由 prior works “A scalable approach to control diverse behaviors for physically simulated characters” 显示，保证说 每个reward都不会被 overlooked.\n\n\n2.3 kinematic aware policy\n这里 RL 的 explore 是定义成 normal distribution 来选择 action 的，这里定义一个 mean 和 variance，mean 在最后应该是 optimal action\n\n\n\n这里  是 kinematic refinement unit, 这里的  就是 kinematic pose after n iterations of refinement. \n 是 control generation unit, 通过当前 pose 和 velocity 和 下一帧的 pose，能得到这些值\n这里没有直接 regress 到 ，因为 author 说这样 learning easier\n2.3.1 kinematic refinement unit \n\n这里定义 MLP ，  是 gradient of reprojection loss , \n\ninspired by prior work “Human body model fitted by learned gradient descent”, 这里不是想 minimize loss，只是想用这个 as informative kinematic feature to learn a pose update 是最后的 pose stable 且 accurate\n\nreprojection loss  定义：\n\n\n大写 X 是 3d 的，小写是 2d 的，这里就是做了一个 projection，并且乘上了 2d joints uncertainty, 来 account for keypoint uncertainty.\n\n z converted to character’s root coordinate to be invariant of character’s orientation\n\n\n文章这里说 因为 dynamic 那边用了 kinematic 的预测，然后在 RL 里面 train 的时候。就类似于 jointly training 了\n\n2.3.2 feature extraction layer这里在 直接把 current pose, pose velocity, 和 next pose 传给 action generation unit 之前，先传给 feature extraction layer, 再经过 normalization layer 之后，再传给 最后的一个 MLP 来生成 action\n\n?????? 这里不应该是 RL 吗，但是这里看到的好像是 MLP 直接 infer？？？todo\n\n2.3.3 Meta-PD control主要关注  和  的 ratio，large ratios lead to unstable and jittery motions, small ratios 可能就太 smooth 然后 lag behind ground truth.\n我们使用两个 initial params  和 ，然后对于每个 15 steps，我们生成参数  and  来得到每一个 step 的 新的  和 \n\n\n\n\n3. Experiments3.1 Metrics检验 shape accuracy 的\n3.1.1 MPJPE (mean per joint position error)\ntodo: …. add formula\n\n3.1.2 PA-MPJPE (procrustes-aligned mean per joint position error)\ntodo; … add formula\n\n检验物理稳定性的\n3.1.3 Accel (difference in accleration between the predicted 3d joint and GT)\nTodo: …. add formula\n\n3.1.4 FS (foot sliding)对比 body mesh vertices that contact the ground on 连续 2 帧，compute average displacement within frames.\n\ntodo:…. add formula\n\n3.1.5 GP (ground penetration)compute average distance to the ground for mesh vertices below the ground.\n\nTodo: …. add formula\n\n3.2 datasets这里用了 human3.6M (S1, S5, S6, S7, S8) 做 training，(S9, S11) 做 testing\n还有 in-house human motion dataset，其中包括了 detailed finger motion.\n3.3 具体的测试3.3.1 simulation character createusing SMPL model for Human3.6M\nFor in-house dataset, use non-rigid ICP and linear blend skinning 来 reconstruct skinned human mesh model.\n\nTodo: 1. non-rigit ICP: optimal step nonrigit icp algo for surface registration; 2. linear blend skinning: skinning with dual quaternions\n\n3.3.2 initializationfor Human3.6M, VIBE 直接 predict pose \nFor in-house dataset, 自己创建了一个 kinematic pose estimator\n3.3.3 Other detailsexperiments 中，之前的 physics-based method 产生很大的 Accel 因为 character 经常 falling when losing balance. (但是其实人没有倒…) \n4. limitations这里因为用了 3d scene simulation 来 enforce contact constraint during motion estimation. 所以对于 in-the-wild dataset 就不能用。\n","categories":["paper review"],"tags":["computer vision","3d human pose","3d human shape","Reinforcement Learning"]},{"title":"end to end human pose and mesh reconstruction with transformers","url":"/2021/10/01/end%20to%20end%20human%20pose%20and%20mesh%20reconstruction%20with%20transformers/","content":"1. 当前方向todo:\n了解 3d 人体的各种表示方法\n了解 SMPL 等方法\n了解 直接 regress 的方法\n了解 masked vertex modeling\n\n1.1 parametric model:利用 ==提前定义好的 parametric model== 像是 SMPL 来 predict shape 和 pose coefficients\n而且提出可以 ==利用其他的检测信息==，类似 skeleton , segmentation map, 其他的 optimization strategies 和 利用 temporal info 来 提升效果\n\n优点：\n有 strong prior about human body, 所以对于 environment 比较 robust\n\n\n缺点：\n受到 定义的 parametric model 的限制，不能很好的 scale 到其他的 task 上，类似 hand-reconstruction 这种\n\n\n\n1.2 直接 regress vertex:考虑利用 ==3D mesh, volumetric space, 或者 occupancy field 来表示3D人体==\n用 Graph CNN 来估计 vertex-vertex interaction, 或者直接估计 vertex coordinates\n\n优点：\n能很好的 scale 到 别的 reconstruction task 上，只要提供足够的数据\n\n\n缺点：\n不是很robust，对于遮挡问题，而且只能找到 local vertex 的关系，但是 non-local vertex 的 关系找不到\n\n\n\n\n2. 方法2.1 workflow\nimage -&gt; CNN 得到 2048-dim vector representation. 然后 和 template 的 joint queries 和 template vertex queries 合并起来。==(CNN 是 pretrained on ImageNet classification task)==\n\n对于每一个要预测的3d点(joint + vertex), 把 一个3d query 和 上面的 feature 合并，然后得到 num_queries x 2051 的 matrix 给 Transformer 做 input.\n\n期间加上 Masked Vertex Modeling (MVM), 类似 dropout. 然后 让 Transformer 预测 全部的 joint 和 vertex.\n\ntransformer 因为 output size 不变，所以这里用 cascade of transformers, 两个之间使用 dimension reduction 的方法 ==(用 encoder 来把 dimension 降下来)==\n\n\n2.2 loss\n$L_v$: 3d vertex 之间的 L1 loss\n$L_j$: 3d joint 之间的 L1 loss\n$L_j^{reg}$: 存在 regression matrix G, 能从 vertex 里面计算出 joint 坐标，这里就是L1 loss of GT joint 和 用 G 算出来的 joint\n$L_J^{proj}$: L1 loss of GT 2d joint point 和 投影出来的 2d joint point ==(这里在 transformer output 上加了一层 linear layer 能自动学习 camera params)==\n\n最后是 $L = \\alpha(L_v + L_j + L_j^{reg}) + \\beta L_J^{proj}$\n==alpha/beta 是 binary flag==, 看看当前的 dataset 有没有 ==3d 或者 2d ground truth.==\n2.3 additional使用 mixed training 2d + 3d (????), todo…\n\n3. experiments3.1 datasets:Human3.6M: 2d + 3d annotations, 但是 3d mesh 有 license 不能用，转而使用 pseudo 3D meshes provided in papers: \n\npose2mesh, ECCV, 2020    \nI2Lmashnet, ECCV, 2020\n\n3DPW: outdoor 2d + 3d annotations, training images 22K, testing 35K.\nUP-3D: outdoor image data, 3d annotations 是 model fitting 创造的, 7K training images.\nMuCo-3DHP: synthesized data from MPI-INF-3DHP dataset, 有很多 real-world background images, 总共 200K training images.\nCOCO: 3d data 是 papers 生成的:\n\nlearning to reconstruct human body pose and shape shape via model-fitting in the loop, ICCV, 2019\n这里使用了 Simplify-X 来 fit 的: expressive body capture: 3d hands, face, and body from a single image, CVPR, 2019\n\n\n\nMPII: outdoor image with 2d poses, 14K training images\nFreiHAND: 3d hand dataset, 130K training, 4K testing.\n\n3.2 Metrics:MPJPE: mean-per-joint-positioning-error ==(用于3d pose)==: euclidean distance between GT joint 和 predicted joints\nPA-MPJPE: ==(一般用于 reconstruction error)==, 用 ==Procrustes Analysis(PA)== 来做 3d alignment，然后再算 MPJPE, 因为和 scale 和 rigid pose 没关系\nMPVE: mean-per-vertex-error, euclidean distance between GT vertex 和 predicted vertex\n\nResults:和众多 SOTA 比，主要使用 Human3.6M (?????) 来对比，都比 SOTA 好\n","categories":["paper review"],"tags":["computer vision","3d human pose","3d human shape","transformers"]},{"title":"Hello World","url":"/2021/10/01/hello-world/","content":"\nWelcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new \"My New Post\"\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["useless"],"tags":["hello_world"]}]