[{"title":"Copy List with Random Pointer","url":"/2021/11/14/leetcode/Copy%20List%20with%20Random%20Pointer/","content":"Copy List with Random Pointer #hardA linked list of length n is given such that each node contains an additional random pointer, which could point to any node in the list, or null.\nConstruct a deep copy of the list. The deep copy should consist of exactly n brand new nodes, where each new node has its value set to the value of its corresponding original node. Both the next and random pointer of the new nodes should point to new nodes in the copied list such that the pointers in the original list and copied list represent the same list state. None of the pointers in the new list should point to nodes in the original list.\nFor example, if there are two nodes X and Y in the original list, where X.random --&gt; Y, then for the corresponding two nodes x and y in the copied list, x.random --&gt; y.\nReturn the head of the copied linked list.\nThe linked list is represented in the input/output as a list of n nodes. Each node is represented as a pair of [val, random_index] where:\n\nval: an integer representing Node.val\nrandom_index: the index of the node (range from 0 to n-1) that the random pointer points to, or null if it does not point to any node.\n\nYour code will only be given the head of the original linked list.\nExample 1:\n\nInput: head = [[7,null],[13,0],[11,4],[10,2],[1,0]]Output: [[7,null],[13,0],[11,4],[10,2],[1,0]]\n\nExample 2:\n\nInput: head = [[1,1],[2,1]]Output: [[1,1],[2,1]]\n\nExample 3:\n\nInput: head = [[3,null],[3,0],[3,null]]Output: [[3,null],[3,0],[3,null]]\n\nExample 4:\nInput: head = []Output: []Explanation: The given linked list is empty (null pointer), so return null.\n\nConstraints:\n\n0 &lt;= n &lt;= 1000\n-10000 &lt;= Node.val &lt;= 10000\nNode.random is null or is pointing to some node in the linked list.\n\nSolution:\nhashmap 来记录 old node to cloned node：O(n)/O(n)\n\n之后不管是怎么 traverse (using next or random, 还是怎么样)就都一样了\n\n\n==在每一个node后面直接创建 next 到 cloned node: O(n)/O(1)==\n\n\n\n\ncloned_node.next = original_node.nextoriginal_node.next = cloned_node\n\n之后对于 random 来做，如果 A.random = B对于 cloned 来说就是 A_cloned.random = B_cloned就是 ==A.next.random = B.next==\n最后 iterate 的时候 next = next.next 即可\n","categories":["leetcode"],"tags":["hard","list"]},{"title":"First Missing Positive","url":"/2021/11/14/leetcode/First%20Missing%20Positive/","content":"First Missing Positive #hardGiven an unsorted integer array nums, return the smallest missing positive integer.\nYou must implement an algorithm that runs in O(n) time and uses constant extra space.\nExample 1:\nInput: nums = [1,2,0]Output: 3\nExample 2:\nInput: nums = [3,4,-1,1]Output: 2\nExample 3:\nInput: nums = [7,8,9,11,12]Output: 1\nConstraints:\n\n  1 &lt;= nums.length &lt;= 5 * 105\n  -231 &lt;= nums[i] &lt;= 231 - 1\n\nSolution:分析问题：对于长度为 n 的 array，最大的 missing positive 可能是几\n\n这个example我们发现是 9，最大为 n+1. 最小为 1.\n这种就考虑用 value hash to index 的方法来做 #value_hash_to_index \n\n对于所有的 &lt;=0 和 &gt;= n+1 的数都设置为 0\n遍历 array，标记 nums[ nums[i] ] = -1 * abs(nums[nums[i]])*，使其为负来标记存在\n最后遍历，寻找 index 里是 非负数的 格子，得到答案\n\nclass Solution:    def firstMissingPositive(self, nums: List[int]) -&gt; int:        n = len(nums)                # Base case.        if 1 not in nums:            return 1                # Replace negative numbers, zeros,        # and numbers larger than n by 1s.        # After this convertion nums will contain         # only positive numbers.        for i in range(n):            if nums[i] &lt;= 0 or nums[i] &gt; n:                nums[i] = 1                # Use index as a hash key and number sign as a presence detector.        # For example, if nums[1] is negative that means that number `1`        # is present in the array.         # If nums[2] is positive - number 2 is missing.        for i in range(n):             a = abs(nums[i])            # If you meet number a in the array - change the sign of a-th element.            # Be careful with duplicates : do it only once.            if a == n:                nums[0] = - abs(nums[0])            else:                nums[a] = - abs(nums[a])                    # Now the index of the first positive number         # is equal to first missing positive.        for i in range(1, n):            if nums[i] &gt; 0:                return i                if nums[0] &gt; 0:            return n                    return n + 1\n\n","categories":["leetcode"],"tags":["hard","array","value2idx"]},{"title":"Largest Rectangle in Histogram","url":"/2021/11/14/leetcode/Largest%20Rectangle%20in%20Histogram/","content":"Largest Rectangle in Histogram #hardGiven an array of integers heights representing the histogram’s bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.\nExample 1:\n\nInput: heights = [2,1,5,6,2,3]Output: 10Explanation: The above is a histogram where width of each bar is 1.The largest rectangle is shown in the red area, which has an area = 10 units.\nExample 2:\n\nInput: heights = [2,4]Output: 4\nConstraints:\n\n  1 &lt;= heights.length &lt;= 105\n  0 &lt;= heights[i] &lt;= 104\n\nSolution:Brute forcetwo pointers i, j; j keeps growing to the right. O(n^2)/O(1)\n==divide and conquer==O(nlogn)/==O(n)== recursion depth O(n)最大的 rectangle 只能是：\n\n当前最长 width * shortest height\n\nshortest height 的左边里面接着找\n\nshortest height 右边接着找\n\n\n  \nclass Solution &#123;public:\tint calculateArea(vector&lt;int&gt;&amp; heights, int start, int end) &#123;\t\tif (start &gt; end) &#123;\t\t\treturn 0;\t\t&#125;\t\tint min_index = start;\t\tfor (int i = start; i &lt;= end; i++) &#123;\t\t\tif (heights[min_index] &gt; heights[i]) &#123;\t\t\t\tmin_index = i;\t\t\t&#125;\t\t&#125;\t\treturn max(&#123;heights[min_index] * (end - start + 1),\t\t\t\t\tcalculateArea(heights, start, min_index - 1),\t\t\t\t\tcalculateArea(heights, min_index + 1, end)&#125;);\t&#125;\tint largestRectangleArea(vector&lt;int&gt;&amp; heights) &#123;\t\treturn calculateArea(heights, 0, heights.size() - 1);\t&#125;&#125;;\n\n\nbetter divide and conquer uses Segment Tree\n\n==### stack method==\n==O(n)/O(n)==\n做 increasing heights stack, 因为 heights 在里面是 increasing 的，如果出现 新的 height 比 stack top 小，则 heights[stack[j]..stack[k]] &gt; new_height ，==这些 heights 所能得到的 最大的 width 就已经确定了，可以计算这些 heights 能得到的 area 了==\n\n注意这里\n\nclass Solution &#123;public:\tint largestRectangleArea(vector&lt;int&gt;&amp; heights) &#123;\t\tstack&lt;int&gt; stk;\t\tstk.push(-1); //代表 end of stack\t\tint max_area = 0;\t\tfor (size_t i = 0; i &lt; heights.size(); i++) &#123;\t\t\twhile (stk.top() != -1 and heights[stk.top()] &gt;= heights[i]) &#123;\t\t\t\tint current_height = heights[stk.top()];\t\t\t\tstk.pop();\t\t\t\tint current_width = i - stk.top() - 1;\t\t\t\tmax_area = max(max_area, current_height * current_width);\t\t\t&#125;\t\t\tstk.push(i);\t\t&#125;\t\twhile (stk.top() != -1) &#123;\t\t\tint current_height = heights[stk.top()];\t\t\tstk.pop();\t\t\tint current_width = heights.size() - stk.top() - 1;\t\t\tmax_area = max(max_area, current_height * current_width);\t\t&#125;\t\treturn max_area;\t&#125;&#125;;\n\n","categories":["leetcode"],"tags":["hard","stack","DP"]},{"title":"Maximal Rectangle","url":"/2021/11/14/leetcode/Maximal%20Rectangle/","content":"Maximal Rectangle #hard [Maximal Square.md](Maximal Square.md) \n [Largest Rectangle in Histogram.md](Largest Rectangle in Histogram.md) \nGiven a rows x cols binary matrix filled with 0‘s and 1‘s, find the largest rectangle containing only 1‘s and return its area.\nExample 1:\n\nInput: matrix = [[“1”,”0”,”1”,”0”,”0”],[“1”,”0”,”1”,”1”,”1”],[“1”,”1”,”1”,”1”,”1”],[“1”,”0”,”0”,”1”,”0”]]Output: 6Explanation: The maximal rectangle is shown in the above picture.\nExample 2:\nInput: matrix = []Output: 0\nExample 3:\nInput: matrix = [[“0”]]Output: 0\nExample 4:\nInput: matrix = [[“1”]]Output: 1\nExample 5:\nInput: matrix = [[“0”,”0”]]Output: 0\nConstraints:\n\n  rows == matrix.length\n  cols == matrix[i].length\n  0 &lt;= row, cols &lt;= 200\n  matrix[i][j] is &#39;0&#39; or &#39;1&#39;.\n\nSolution:DP solution:注意这里和 maximal square 的不同，这里 DP 不能很好地显示 diagonal 的情况，O(mn^2)/O(mn)\n考虑 dp[i,j] 记录下当前 row 的到这里为止的 最长的 rect width之后 寻找 maximal area 的时候，\n我们从 0...i-1 看看，所能达到的最大的 area 是多少。重复这个步骤\ndef maximalRectangle(self, matrix: List[List[str]]) -&gt; int:    maxarea = 0    dp = [[0] * len(matrix[0]) for _ in range(len(matrix))]    for i in range(len(matrix)):        for j in range(len(matrix[0])):            if matrix[i][j] == &#x27;0&#x27;: continue            # compute the maximum width and update dp with it            width = dp[i][j] = dp[i][j-1] + 1 if j else 1            # compute the maximum area rectangle with a lower right corner at [i, j]            for k in range(i, -1, -1):                width = min(width, dp[k][j])                maxarea = max(maxarea, width * (i-k+1))    return maxarea\n\n\n\n==Histograms== [Largest Rectangle in Histogram.md](Largest Rectangle in Histogram.md) \nO(mn)/O(n)\n依照上面的方法得到 dp，每一个 column 往上都是一个 histogram，histogram 找 largest 需要 O(n), 所以总共 需要 O(2mn) \n\n计算右边的 histograms 就直接加减就行\n\n# Get the maximum area in a histogram given its heights def leetcode84(self, heights):     stack = [-1]     maxarea = 0     for i in range(len(heights)):         while stack[-1] != -1 and heights[stack[-1]] &gt;= heights[i]:             maxarea = max(maxarea, heights[stack.pop()] * (i - stack[-1] - 1))         stack.append(i)     while stack[-1] != -1:         maxarea = max(maxarea, heights[stack.pop()] * (len(heights) - stack[-1] - 1))     return maxarea\n\n\ndef maximalRectangle(self, matrix: List[List[str]]) -&gt; int:\n\n    if not matrix: return 0\n\n    maxarea = 0\n    dp = [0] * len(matrix[0])\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n\n            # update the state of this row&#39;s histogram using the last row&#39;s histogram\n            # by keeping track of the number of consecutive ones\n\n            dp[j] = dp[j] + 1 if matrix[i][j] == &#39;1&#39; else 0\n\n        # update maxarea with the maximum area from this row&#39;s histogram\n        maxarea = max(maxarea, self.leetcode84(dp))\n    return maxarea\n\n### ==DP solution better==**O(mn)/O(m)**idea 就是构建三个 DP table of size m x n:1. heights: 记录当前 slot 往上最高的 height2. left: 记录当前 slot 的高度能延伸到的最左边的 index3. right: 记录当前 slot 的高度能延伸到的最右边的 index**heights init 成 0，left init 成 0，right init 成 n &lt;u&gt;(#column)&lt;/u&gt;**==**Update Rule (都只 apply to slot with value = 1):**==1. `height[i+1][j] = height[i][j] + 1  `2. `left[i+1][j] = max(left[i][j], current_left) `3. `right[i+1][j] = min(right[i][j], current_right)`&gt; current_left 就是 当前 row 能到的最左边&gt;&gt; current_right 就是当前 row 能到的最右边下面展示的代码是 把上面的 space 优化的结果，因为 DP 只联系了相邻两层，所以只需要 **n-sized vector** 即可```pythondef maximalRectangle(self, matrix: List[List[str]]) -&gt; int:     if not matrix: return 0     m = len(matrix)     n = len(matrix[0])     left = [0] * n # initialize left as the leftmost boundary possible     right = [n] * n # initialize right as the rightmost boundary possible     height = [0] * n     maxarea = 0     for i in range(m):         cur_left, cur_right = 0, n         # update height         for j in range(n):             if matrix[i][j] == &#x27;1&#x27;: height[j] += 1             else: height[j] = 0         # update left         for j in range(n):             if matrix[i][j] == &#x27;1&#x27;: left[j] = max(left[j], cur_left)             else:                 left[j] = 0                 cur_left = j + 1         # update right         for j in range(n-1, -1, -1):             if matrix[i][j] == &#x27;1&#x27;: right[j] = min(right[j], cur_right)             else:                 right[j] = n                 cur_right = j         # update the area         for j in range(n):             maxarea = max(maxarea, height[j] * (right[j] - left[j]))     return maxarea\n\n","categories":["leetcode"],"tags":["hard","stack","DP"]},{"title":"Maximal Rectangle","url":"/2021/11/14/leetcode/Maximal%20Square/","content":"Maximal Square #mediumGiven an m x n binary matrix filled with 0‘s and 1‘s, find the largest square containing only 1‘s and return its area.\nExample 1:\n\nInput: matrix = [[“1”,”0”,”1”,”0”,”0”],[“1”,”0”,”1”,”1”,”1”],[“1”,”1”,”1”,”1”,”1”],[“1”,”0”,”0”,”1”,”0”]]Output: 4\nExample 2:\n\nInput: matrix = [[“0”,”1”],[“1”,”0”]]Output: 1\nExample 3:\nInput: matrix = [[“0”]]Output: 0\nConstraints:\n\n  m == matrix.length\n  n == matrix[i].length\n  1 &lt;= m, n &lt;= 300\n  matrix[i][j] is &#39;0&#39; or &#39;1&#39;.\n\nSolutionDP直接用 DP  O(mn)/O(mn)==space can be minimized to O(n)，因为每次只考虑相邻两行==\n每个 dp[i,j] 都记录当前形成的最大的 square 的 area 记录在表里面，然后我们 从左往右，从上往下 遍历\n对于 dp[i,j]，\n\n如果 matrix[i,j] == 1 我们通过画图可以得到 dp[i,j] = min(dp[i-1,j], dp[i-1,j-1], dp[i,j-1]) + 1 其余的就 ignore。\n\npublic class Solution &#123;    public int maximalSquare(char[][] matrix) &#123;        int rows = matrix.length, cols = rows &gt; 0 ? matrix[0].length : 0;        int[] dp = new int[cols + 1];        int maxsqlen = 0, prev = 0;        for (int i = 1; i &lt;= rows; i++) &#123;            for (int j = 1; j &lt;= cols; j++) &#123;                int temp = dp[j];                if (matrix[i - 1][j - 1] == &#x27;1&#x27;) &#123;                    dp[j] = Math.min(Math.min(dp[j - 1], prev), dp[j]) + 1;                    maxsqlen = Math.max(maxsqlen, dp[j]);                &#125; else &#123;                    dp[j] = 0;                &#125;                prev = temp;            &#125;        &#125;        return maxsqlen * maxsqlen;    &#125;&#125;","categories":["leetcode"],"tags":["hard","DP"]},{"title":"Minimum Window Substring","url":"/2021/11/14/leetcode/Minimum%20Window%20Substring/","content":"Minimum Window Substring #hardGiven two strings s and t of lengths m and n respectively, return the minimum window substring of s such that every character in t (including duplicates) is included in the window. If there is no such substring, return the empty string \"\".\nThe testcases will be generated such that the answer is unique.\nA substring is a contiguous sequence of characters within the string.\nExample 1:\nInput: s = “ADOBECODEBANC”, t = “ABC”Output: “BANC”Explanation: The minimum window substring “BANC” includes ‘A’, ‘B’, and ‘C’ from string t.\nExample 2:\nInput: s = “a”, t = “a”Output: “a”Explanation: The entire string s is the minimum window.\nExample 3:\nInput: s = “a”, t = “aa”Output: “”Explanation: Both ‘a’s from t must be included in the window.Since the largest window of s only has one ‘a’, return empty string.\nConstraints:\n\n  m == s.length\n  n == t.length\n  1 &lt;= m, n &lt;= 105\n  s and t consist of uppercase and lowercase English letters.\n\nFollow up: Could you find an algorithm that runs in O(m + n) time?\nSolution:\n首先是对比 s 是不是包含 t 的方式，我们用 hashmap 里面存着 wordcount。另外有 total counter 来记录是不是都 include 了\n==sliding window 的办法来选取最小的 window，two pointers i, j：==\n当 s[i:j] 不满足条件时，j++；\n==当 s[i:j]满足条件时，i++ 直到刚刚不满足条件；==\n\n\n\nRegular Sliding WindowO(S+T)/O(S+T)\ndef minWindow(self, s, t):    \"\"\"    :type s: str    :type t: str    :rtype: str    \"\"\"    if not t or not s:        return \"\"    # Dictionary which keeps a count of all the unique characters in t.    dict_t = Counter(t)    # Number of unique characters in t, which need to be present in the desired window.    required = len(dict_t)    # left and right pointer    l, r = 0, 0    # formed is used to keep track of how many unique characters in t are present in the current window in its desired frequency.    # e.g. if t is \"AABC\" then the window must have two A's, one B and one C. Thus formed would be = 3 when all these conditions are met.    formed = 0    # Dictionary which keeps a count of all the unique characters in the current window.    window_counts = {}    # ans tuple of the form (window length, left, right)    ans = float(\"inf\"), None, None    while r &lt; len(s):        # Add one character from the right to the window        character = s[r]        window_counts[character] = window_counts.get(character, 0) + 1        # If the frequency of the current character added equals to the desired count in t then increment the formed count by 1.        if character in dict_t and window_counts[character] == dict_t[character]:            formed += 1        # Try and contract the window till the point where it ceases to be 'desirable'.        while l &lt;= r and formed == required:            character = s[l]            # Save the smallest window until now.            if r - l + 1 &lt; ans[0]:                ans = (r - l + 1, l, r)            # The character at the position pointed by the `left` pointer is no longer a part of the window.            window_counts[character] -= 1            if character in dict_t and window_counts[character] &lt; dict_t[character]:                formed -= 1            # Move the left pointer ahead, this would help to look for a new window.            l += 1            # Keep expanding the window once we are done contracting.        r += 1        return \"\" if ans[0] == float(\"inf\") else s[ans[1] : ans[2] + 1]\n\n\n\n==Optimized Sliding Window (build char2idx map)==O(2*filtered_S + S +T)/O(S + T)\n==对于 s 很长且很多 char 不在 t 中，构建 filtered_S, filtered_S 里面只包含了 t 中出现的 chars，之后都一样了，之后 iterate filtered_S 就更快了== \n  S = \"ABCDDDDDDEEAFFBC\" T = \"ABC\"filtered_S = [(0, 'A'), (1, 'B'), (2, 'C'), (11, 'A'), (14, 'B'), (15, 'C')]Here (0, 'A') means in string S character A is at index 0.\n\ndef minWindow(self, s, t):    \"\"\"    :type s: str    :type t: str    :rtype: str    \"\"\"    if not t or not s:        return \"\"    dict_t = Counter(t)    required = len(dict_t)    # Filter all the characters from s into a new list along with their index.    # The filtering criteria is that the character should be present in t.    filtered_s = []    for i, char in enumerate(s):        if char in dict_t:            filtered_s.append((i, char))    l, r = 0, 0    formed = 0    window_counts = {}    ans = float(\"inf\"), None, None    # Look for the characters only in the filtered list instead of entire s. This helps to reduce our search.    # Hence, we follow the sliding window approach on as small list.    while r &lt; len(filtered_s):        character = filtered_s[r][1]        window_counts[character] = window_counts.get(character, 0) + 1        if window_counts[character] == dict_t[character]:            formed += 1        # If the current window has all the characters in desired frequencies i.e. t is present in the window        while l &lt;= r and formed == required:            character = filtered_s[l][1]            # Save the smallest window until now.            end = filtered_s[r][0]            start = filtered_s[l][0]            if end - start + 1 &lt; ans[0]:                ans = (end - start + 1, start, end)            window_counts[character] -= 1            if window_counts[character] &lt; dict_t[character]:                formed -= 1            l += 1            r += 1        return \"\" if ans[0] == float(\"inf\") else s[ans[1] : ans[2] + 1]\n\n","categories":["leetcode"],"tags":["hard","sliding window","char2idx map"]},{"title":"Next Permutation","url":"/2021/11/14/leetcode/Next%20Permutation/","content":"Next PermutationImplement next permutation, which rearranges numbers into the lexicographically next greater permutation of numbers.\nIf such an arrangement is not possible, it must rearrange it as the lowest possible order (i.e., sorted in ascending order).\nThe replacement must be in place and use only constant extra memory.\nExample 1:\nInput: nums = [1,2,3]Output: [1,3,2]\n\nExample 2:\nInput: nums = [3,2,1]Output: [1,2,3]\n\nExample 3:\nInput: nums = [1,1,5]Output: [1,5,1]\n\nExample 4:\nInput: nums = [1]Output: [1]\n\nConstraints:\n\n1 &lt;= nums.length &lt;= 100\n0 &lt;= nums[i] &lt;= 100\n\nSolution==Make up some examples of numbers==, and the algorithm for finding the next permutations is:\n\nIterate from right to left. If the number is increasing, the sub-number we have seen is in its largest. \nFind the first index $i$ where the in adjaceny,  left number num[i-1] is greater than the right number num[i]. Swap these two numbers. We are sure that num[0]..num[i],num[i-1],... must share the first $i$ values with the smallest permutation 不可能更小了\nThen, we can make sure num[i+1],...num[n] are the smallest by reversing them. (因为我们刚才从右到左都是 increasing，现在可以让他们从左到右都是increasing即可)\n\npublic class Solution &#123;    public void nextPermutation(int[] nums) &#123;        int i = nums.length - 2;        while (i &gt;= 0 &amp;&amp; nums[i + 1] &lt;= nums[i]) &#123;            i--;        &#125;        if (i &gt;= 0) &#123;            int j = nums.length - 1;            while (j &gt;= 0 &amp;&amp; nums[j] &lt;= nums[i]) &#123;                j--;            &#125;            swap(nums, i, j);        &#125;        reverse(nums, i + 1);    &#125;    private void reverse(int[] nums, int start) &#123;        int i = start, j = nums.length - 1;        while (i &lt; j) &#123;            swap(nums, i, j);            i++;            j--;        &#125;    &#125;    private void swap(int[] nums, int i, int j) &#123;        int temp = nums[i];        nums[i] = nums[j];        nums[j] = temp;    &#125;&#125;\n\n\n\n\n\n","categories":["leetcode"],"tags":["array","medium"]},{"title":"Regular Expression Matching","url":"/2021/11/14/leetcode/Regular%20Expression%20Matching/","content":"Regular Expression Matching #hardGiven an input string s and a pattern p, implement regular expression matching with support for &#39;.&#39; and &#39;*&#39; where:\n\n  &#39;.&#39; Matches any single character.​​​​\n  &#39;*&#39; Matches zero or more of the preceding element.\n\nThe matching should cover the entire input string (not partial).\nExample 1:\nInput: s = “aa”, p = “a”Output: falseExplanation: “a” does not match the entire string “aa”.\nExample 2:\nInput: s = “aa”, p = “a*”Output: trueExplanation: ‘*’ means zero or more of the preceding element, ‘a’. Therefore, by repeating ‘a’ once, it becomes “aa”.\nExample 3:\nInput: s = “ab”, p = “.“Output: trueExplanation: “.“ means “zero or more (*) of any character (.)”.\nExample 4:\nInput: s = “aab”, p = “cab”Output: trueExplanation: c can be repeated 0 times, a can be repeated 1 time. Therefore, it matches “aab”.\nExample 5:\nInput: s = “mississippi”, p = “misisp*.”Output: false\nConstraints:\n\n  1 &lt;= s.length &lt;= 20\n  1 &lt;= p.length &lt;= 30\n  s contains only lowercase English letters.\n  p contains only lowercase English letters, &#39;.&#39;, and &#39;*&#39;.\n  It is guaranteed for each appearance of the character &#39;*&#39;, there will be a previous valid character to match.\n\nSolution:recursion 来做 infinite state machine 来检查有没有 match 的可能:![](Regular Expression Matching.assets/RE_match_recurse.png)\n这里介绍 recursion 的简便写法：\npattern[i:] 和 string[i:] 能不能 match, 取决于：\n\npattern[i] 和 string[i] 能不能match (相同或者pattern[i]是 . )\n如果 pattern[i+1] 是*，则考虑：\n跳过 pattern 的 * 和 剩下的 string 对比\n保留 pattern i 到 * 和剩下的string 对比\n\n\n如果不是，接着各自往下对比即可\n\nclass Solution(object):\tdef isMatch(self, text, pattern):\t\tif not pattern:\t\t\treturn not text\t\tfirst_match = bool(text) and pattern[0] in &#123;text[0], &#x27;.&#x27;&#125;\t\tif len(pattern) &gt;= 2 and pattern[1] == &#x27;*&#x27;:\t\t\treturn (self.isMatch(text, pattern[2:]) or\t\t\t\t\tfirst_match and self.isMatch(text[1:], pattern))\t\telse:\t\t\treturn first_match and self.isMatch(text[1:], pattern[1:])\n\n\n\nDP==O(TP)/O(TP)== \nT: len of string, P: len of patterns\n\ntop down approach：class Solution(object): def isMatch(self, text, pattern):     memo = &#123;&#125;     def dp(i, j):         if (i, j) not in memo:             if j == len(pattern):                 ans = i == len(text)             else:                 first_match = i &lt; len(text) and pattern[j] in &#123;text[i], &#x27;.&#x27;&#125;                 if j+1 &lt; len(pattern) and pattern[j+1] == &#x27;*&#x27;:                     ans = dp(i, j+2) or first_match and dp(i+1, j)                 else:                     ans = first_match and dp(i+1, j+1)             memo[i, j] = ans         return memo[i, j]     return dp(0, 0)\n\n\nidea 就是用 (i,j) idx 来表示有没有 match 到, 然后一直往下找，直到 i=T and j=P\n\n\n==bottom up approach (better without recursion)==\n\nclass Solution(object): def isMatch(self, text, pattern):     dp = [[False] * (len(pattern) + 1) for _ in range(len(text) + 1)]     dp[-1][-1] = True     for i in range(len(text), -1, -1):         for j in range(len(pattern) - 1, -1, -1):          \t # pattern[j] match pattern[i]             first_match = i &lt; len(text) and pattern[j] in &#123;text[i], &#x27;.&#x27;&#125;             if j+1 &lt; len(pattern) and pattern[j+1] == &#x27;*&#x27;:              \t # 1. * can be zero length, text[i] match pattern[j+2]                 # 2. first match                 dp[i][j] = dp[i][j+2] or first_match and dp[i+1][j]             else:                 dp[i][j] = first_match and dp[i+1][j+1]     return dp[0][0]\n\n","categories":["leetcode"],"tags":["hard","DP","recursion"]},{"title":"Median of two sorted arrays","url":"/2021/11/14/leetcode/median%20of%20two%20sorted%20arrays/","content":"Median of two sorted arrays #hardGiven two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.\nThe overall run time complexity should be O(log (m+n)).\nExample 1:\nInput: nums1 = [1,3], nums2 = [2]Output: 2.00000Explanation: merged array = [1,2,3] and median is 2.\n\nExample 2:\nInput: nums1 = [1,2], nums2 = [3,4]Output: 2.50000Explanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.\n\nExample 3:\nInput: nums1 = [0,0], nums2 = [0,0]Output: 0.00000\n\nExample 4:\nInput: nums1 = [], nums2 = [1]Output: 1.00000\n\nExample 5:\nInput: nums1 = [2], nums2 = []Output: 2.00000\n\nConstraints:\n\nnums1.length == m\nnums2.length == n\n0 &lt;= m &lt;= 1000\n0 &lt;= n &lt;= 1000\n1 &lt;= m + n &lt;= 2000\n-106 &lt;= nums1[i], nums2[i] &lt;= 106\n\nSolution:\n如果brute force 我们直接 merge，得到median就 O(m+n). 看到题目要求 O(log(m+n)), 知道应该使用 binary search. \n==考虑，如果我们在 array A 中找到一个index i，在 array B 中对应的 index j（median或median旁边），必须满足 i + j = (m + n + 1)/2. 而且，必须满足 A[i-1] &lt; B[j] 且 B[j-1] &lt; A[i]。==\n如果 A[i-1] &gt; B[j], 证明说 A[i] 会比真正 median 大，所以 i 减小；反之亦然。\n\n总结起来就是这个图片：\n![1_exp](median of two sorted arrays.assets/1_exp.png)\n结合以上的点，我们选择在 array A 里面搜索，然后找到 对应的 array B 中的 index j, 之后接着判断。 \n==这里我们因为 j = (m+n+1)/2 - i 得到，保证 j &gt;= 0, 所以需要 m &lt;= n。而且也加快搜索速度！！！== \ndef findMedianSortedArrays(self, A, B):        m, n = len(A), len(B)        if m &gt; n:            A, B, m, n = B, A, n, m        # if n == 0:        #     raise ValueError        imin, imax, half_len = 0, m, (m + n + 1) / 2        while imin &lt;= imax:            i = (imin + imax) / 2            j = half_len - i            if i &lt; m and B[j-1] &gt; A[i]:                # i is too small, must increase it                imin = i + 1            elif i &gt; 0 and A[i-1] &gt; B[j]:                # i is too big, must decrease it                imax = i - 1            else:                # i is perfect                if i == 0: max_of_left = B[j-1]                elif j == 0: max_of_left = A[i-1]                else: max_of_left = max(A[i-1], B[j-1])                                    if (m + n) % 2 == 1:                    return max_of_left                if i == m: min_of_right = B[j]                elif j == n: min_of_right = A[i]                else: min_of_right = min(A[i], B[j])                return (max_of_left + min_of_right) / 2.0\n\n==注意这里 i 的搜索范围是 [0,m] (0,m 都包括), 因为 i 类似于 cut，i=0 代表 A[0] 也在 right_part, i=m 代表 A[m-1] 在 left_part==\n因为 (m+n+1)/2 保证了左边比右边长，所以如果奇数的话，max_of_left 就是 median\n","categories":["leetcode"],"tags":["hard","array","pointer","binary search"]},{"title":"Trapping Rain Water","url":"/2021/11/14/leetcode/trapping%20rain%20water/","content":"Trapping Rain Water #hardGiven n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining.\nExample 1:\n\nInput: height = [0,1,0,2,1,0,1,3,2,1,2,1]Output: 6Explanation: The above elevation map (black section) is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped.\n\nExample 2:\nInput: height = [4,2,0,3,2,5]Output: 9\n\n\nConstraints:\n\nn == height.length\n1 &lt;= n &lt;= 2 * 104\n0 &lt;= height[i] &lt;= 105\n\nSolution:Brute-force==先考虑 brute-force, 对于每一个点，我们考虑左边的最高的墙和右边的最高的墙，最终能hold的水为 min(left_height, right_height) - curr_height. 这样就 O(n^2)/O(1)==\nDPBrute-force 基础上我们用 DP 存下左边到右边的高度，和右边到左边的高度，之后直接求. O(n)/O(n)\nStack用一个 stack 来做，观察到如果有 valley (本来都是 decreasing heights, 看到比 stk 顶部更大的 height)，我们会去iterate直到比当前 height 高的时候，这个时候我们更新得到的water，同时把这个valley弄平掉。O(n)/O(n), faster than 2., in one pass.\n\n 我们往 stack 中添加 index，如果比当前 index 的 height 低，则继续添加；如果比 top() 的 height 大，证明出现了valley，pop 出小的部分，计算得到的水量计入总量；\n\nint trap(vector&lt;int&gt;&amp; height)&#123;   int ans = 0, current = 0;   stack&lt;int&gt; st;   while (current &lt; height.size()) &#123;\t   while (!st.empty() &amp;&amp; height[current] &gt; height[st.top()]) &#123;\t\t   int top = st.top();\t\t   st.pop();\t\t   if (st.empty())\t\t\t   break;\t\t   int distance = current - st.top() - 1;\t\t   int bounded_height = min(height[current], height[st.top()]) - height[top];\t\t   ans += distance * bounded_height;\t   &#125;\t   st.push(current++);   &#125;   return ans;&#125;\n\n\n\n==two pointers approach: O(n)/O(1)==Best explained here:\nhttps://youtu.be/XqTBrQYYUcc\n总结就是：\n\n 我们对于每个 index 都在求函数 ==g(i) = min(left_max(i), right_max(i) )==。这个类似于求 lower envelop, ==遇到类似lower envelop问题就是要想到 two pointers 或者 binary search==，这里介绍 two pointers 办法\n\n\n 从左到右的顺序下，利用 left_max 函数 non-decreasing 和 right_max 函数 non-increasing 的性质，每次对比决定 pointer i 还是 pointer j 往中间移动。\n\nclass Solution &#123;    public int trap(int[] height) &#123;        // time : O(n)        // space : O(1)        if (height.length==0) return 0;         int left = 0, right = height.length-1;         int leftMax=0, rightMax=0;         int ans = 0;         while (left &lt; right) &#123;            if (height[left] &gt; leftMax) leftMax = height[left];             if (height[right] &gt; rightMax) rightMax = height[right];          \t// 因为 rightMax 越靠左越大，所以这里一定能 trap 住了          \t// 这左边的 water 肯定能被 trap 了            if (leftMax &lt; rightMax) &#123;                ans += Math.max(0, leftMax-height[left]);                 left++;             &#125; else &#123;                ans += Math.max(0, rightMax-height[right]);                 right--;             &#125;        &#125;        return ans;     &#125;&#125;\n\n\n\n\n\n","categories":["leetcode"],"tags":["hard","stack","DP","pointers"]},{"title":"Analyzing and Preventing Sandwich Attacks in Ethereum","url":"/2021/11/14/paper_review/Analyzing%20and%20Preventing%20Sandwich%20Attacks%20in%20Ethereum/","content":"1. Abstract1.1 Front-running在传统 trading 里面，如果对一件东西的需求提高，这件东西就会升值，如果这时候 有人得到内部消息(insider information), 知道说有人准备买入大量的 x，我就可以先买一些 x，然后坐等手上的 x 升值，之后再卖出去。\n在 DeFi 里面，DEX (decentralized exchange) 是用来交换不同种的 crypto 的，同样遵循价格随需求上升而上升的规律，这个时候如果有 node 接收到了 有大额买入的 transaction, 他可以在 这个 transaction 发生之前也买入 一些，要保证他的这个操作发生在 transaction 之前，有几种做法:\n\nnode 就是 miner，那么可以直接把自己的 买入 放进去，然后把 transaction 放到 block 的后面\nnode 希望 别的 miner 能提前发布自己的 买入，则把 incentive (gas price) 调高一些，那么miners就会更先 mine 这个 node 的 买入\n\n2. IntroFlashbots [5] which allows users to submit suggestions for block compositions directly to miners.\n\n软件 Flashbots 能提供合适的 combination or ordering of transactions in a block 来获得收益\n\nMEV (miner extractable value): a measure of the profit a miner (or validator, sequencer, etc.) can make through their ability to arbitrarily include, exclude, or re-order transactions within the blocks they produce.\n当前的解决办法是 private mempools, 然后直接付钱给miners, 但是这个就不是 decentrialized, 而且 cost 很高\n\n这里的思考是，给 incentives 也是没用的，因为 users 在其中是有损失的，如果 frontrunning 只是 赚钱的话那提出 incentives 啥的还行，但是这里有损失的话可能就不太行了。**(单纯的坏人)**\n\n\n这个问题比较严重，因为这里 frontrunner 不算是 dishonest users, 因为不算 break rules\n\n3. background3.1 AMM (automated market maker)3.1.1 liquidity pool[liquidity pool.md](liquidity pool.md) \nliquidity pool, 用来方便交易\nliquidity provider 类似于 order book model 里面的 market maker 愿意提供资金来买交易中的东西\n3.1.2 constant product AMM (UniSwap)Users are able to swap tokens in the liquidity pool if the product of the two reserves,  and , is the same before and after the swap. Most AMMs also charge a fee for executing trades (currently 0.3% of the input amount on Uniswap). This fee stays in the asset pool and is eventually distributed to the liquidity providers as incentive to provide liquidity.\n\n如果 pool 里面的 product of the two reserves are the same, 那就能够 swap tokens.\nMost AMMs 都会收 0.3% fee for executing trades. 最后变成 激励 liquidity providers to provide liquidity.\n\nIf a user swaps an amount  of asset  on Uniswap, the output amount of asset  is hence calculated using the formula:\n\n\n\nr1, r2 都是 liquidity pool 里面的 资金数量，a 就是 trade 里面的 token 数量\n这里就是说 如果 想交易 a 数量的 token，这个就是**==最后实际能买到的 token 的数量，因为升值了==**\n\n因为每笔交易都为引起 价格变化，在发起交易的 user 那边，可以设置 maximum relative price increase, 如果在买之前已经升值超过了这个值，则交易失败\n==slippage rate==: 本来是预估价和实际成交价之间的差值，现在变成 衡量 上面的 maximum relative price 的，但是实际上会被用到的值，**==代表了 最少 被交换到的 数量==，如果上面公式计算出来的最后收到的 数量比 slippage rate 要低，则交易失败**\n\n但是 gas fee 还是要交的，这里的 0.997 就是因为 0.3% 交了 gas\n\n3.2 sandwich attacks其实就是 front-running + back-running, 有时候直接整个叫做 front-running.\nback-running 就类似于知道 X 升值了，马上的卖出 X 的操作，因为希望马上卖出 X，所以可能就会用 更高的 gas price 之类的 incentive 操作来使得 back-running 的操作先执行。\n\n本来这种操作没有问题，还会帮助使 price across markets stable. 但是这种操作会对网络产生 congestion, 会使得很多有用的 transaction 发布不出去，这个问题也叫做 transaction spam\n\nThere is a current discussion between experts whether the existence of MEV is a risk for consensus layer security [3], or fundamental for keeping the Ethereum network decentralized[6].\n\n但是 researchers 内部还是对于 front-running 这个东西是好是坏有争议的\n\n3.2.1 profit of attackers我们这里把 正常的大额买入叫做 victim transaction.\n这种 sandwich attack 是被 slippage rate 给限制的，比如说有可能有很多 front-runner, 他们都同时买入了 X，导致 X 在 victim transaction happen 之前已经产生最后 output amount lower than the slippage rate, 那么 victim transaction 就不执行了，attackers 就没收益了.\n用公式来说明上面条件就是:\n\n\n\n注意这里 highlight 部分，就是 attackers 用 x tokens 买了的 output amount\n is the amount of asset  to be swapped in victim transaction,  are the reserves for assets \n is the slippage rate of victim transaction\n==这个公式就是 把 r_2 减去 attacker 换走的数量，再把 r_1 加上 attacker 放进来的数量，之后再通过公式计算即可==\n\n上面不等式取等号就能算出 attackers 能投的最大的数量 (超过的话，victim transaction 就执行不了)\n\n\n\n\n4. Analysis method观察 ETH 的历史 transactions，identify sandwich attacks, 用了以下的 heuristic 来判断是不是 sandwich attacks:\n\n\n\n总结就是找一对 transaction, 数量相同，方向相反，同一个 block 内，且不同对 transactions 之间不能重叠\n作者认为 Rule 1 保证了能找到 sandwich attacks 的数量的 lower bound， \nRule 2 也是必须的因为存在 一个 transaction 里面同时 swap 两种 assets 的，但是因为没用 victim transaction 在中间，所以不可能是 sandwich attack\nEqual amount 其实也不一定，可以买多一些然后卖少一些都行，但是这里先找lower bound，全部卖掉也是当前attacker的最佳策略\n\n4.1 differ from prior works这里不需要在两次 attacks 之间寻找 victim transaction, 因为 victim transaction 可能会 fail, 导致 sandwich attacks 失败\n之前的 research 要求 liquidity for the two swaps must be provided by the same address. \n还有的要求 两个相反的 transaction 必须是 signed by same account, 而且 sent to same smart contract\n\n作者认为有 more sophisticated attacks 了现在所以 上面不一定 hold\n\n还有 认为 gas price of victim transaction must be between front-running transaction and back-running transaction\n\n但是作者认为 miner 现在也可能是 sandwich attacker，对于自己mine出来的 transaction，就不需要 gas fee 了\n\n5. empirical result发现 attackers 也并不是稳定获利的，attackers 的 transaction 可能会被 miner 给利用，来 reorder transaction in unfavorably order.\n比如说 attacker 和 miner 都看见了 victim transaction 买入 asset X\nattacker 发送了 high gas price 买入 X，和 卖出 X 的 操作，miner 如果把 attacker 的操作顺序倒过来 (在 attackers 手上有 X 的时候)，那么 买入 X 的操作 miner 反而还能以更低的价格来买入\nattacker 在这种情况下就会承受损失了\nDaian [6] even argues that this process is fundamental to keep the Ethereum network secure in the future.\n\n所以也有 researcher 认为 miner 的这种操作其实也保证了 chain 的稳定\n我这里的理解是，因为 miner is incentivized to reorder transactions, 所以只要出现能在 自己买之前把 asset X 的价格降下来的transaction 都放到前面去，然后自己买入，坐等升值。首先在一定程度上，attackers 也冒着风险了\n但是不管 miner 和 attackers 之间的博弈怎么样，victim transactions 大概率都是损失了\nvictim transactions 有可能获利的情况是, total attackers selling outrun victim transactions buying, 导致价格还没 victim transaction 预估的高，几率还没计算，todo!!!!\n\n==思考： 把 X 的价格先降下去，再通过 victim transaction 提升的价格是会更赚钱吗？==\n是的，因为 价格变化 是和 reserve 大小有关的，reserve 变少了，同样数量的买入，提升价格的能力就提升了，所以会更赚，所以我们发现其实上面 计算的 maxInput 还可以更大，因为把 别人的卖出操作放在 miner 的 front-running 前面还有好处，能够降低 低于 slippage rate 的风险\n6. order splitting as solutionMain Idea of front-running attacks 就是:\n==A sandwich attack is only possible if the difference in market price before and after the swap is large enough to compensate for transaction and exchange fees.==\n作者认为只要把 这种 price change 影响降低到够低，就避免了 front-running attacks\nThe number of split orders must be kept as low as possible - not only because of the constant transaction fee which needs to be paid for every transaction, but also because the exchange fee stays in the pool after every swap and changes the market price for the worse\n\n作者希望使 number of splitted transactions 降到最低，因为 transaction fee 是 constant for each transaction，而且 transaction fee 会被放进 reserve 里面最终仍然会提高价格\n\n\n\n\n这里  计算了 attacker 把 assets 再卖出所得到的 token 数量\nv 是 victim transaction 要买的 token 数量\n我们把 v 分成  之后再回来算，就能知道 lower bound to prevent front-running, 也能算出 trade-offs\n\n","categories":["paper review"],"tags":["DeFi","front-running"]},{"title":"Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video","url":"/2021/11/14/paper_review/Beyond%20Static%20Features%20for%20Temporally%20Consistent%203D%20Human%20Pose%20and%20Shape%20from%20a%20Video/","content":"Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video这里开头说的 temporally consistent and smooth 3D human motion from video 应该意思就是能够生成 consistent pose and shape\n1. intro用 video 学习的方法\n\nencode each frame into img_feat\nsend img_feat of all frames to temporal encoder to get temporal_feat\nsend temporal_feat to SMPL param regressor to regress values\n\n但是这上面的方法上面还是有很多的 temporal inconsistency\n\n作者认为类似这种的方法对于 current frame 的 dependency 太强，导致了 temporal learning 没学到\n\n\n作者特别 criticize VIBE 里面的 residual connections 把 current frame’s img_feat 直接 拼到 the final temporal_feat 上面给 SMPL regressor 是导致只学习到 current frame 的问题所在\n\n\n中心思想 就是 强制 模型去学习 temporal features\n\n作者的 解决思路是 提出了 PoseForecast 模组\n中心思想就是 训练模型，能从 past frames 和 future frames 里面 infer current frame, 这样就类似于强制模型学习 temporal information\n之后把这个模组生成的 feature 给 integrate 起来最后做 SMPL parameter regression\n2. existing works\nHMR: SMPL + adversarial loss 来使 mesh anatomically plausible (???????)\n\nHMMR: hallucinator(???): quote, “It hallucinates the past and future 3D poses from a current frame and is self-supervised by the output of the 1D fully convolutional temporal encoder”. 我这里的理解是不是 类似 siamese network??\n\n2D joint heatmaps and silhouettes to predict SMPL params\n\nself-improving system consisting of SMPL regressor and iterative fitting network\n\n应该就类似 RNN，把生成的 output 重新作为 input feed 进去，多次 fitting\n\n\n类似 SMPL 的，用 template human mesh \n\nlixel-based 1D heatmap 来 localize mesh vertices\n\noptical flow + 2D poses 来使 模型 ==generalize to unseen video==\n\nenforce network to re-order shuffled frames 来 enforce learning temporal info\n\n==motion discriminator 来保证 plausible human motion==\n\nMEVA: coarse to find estimation: estimate coarse 3D human motion then predict residual motion to refine\n\npredict future 2D poses, 再从 2D poses 上面 predict 3D poses\n\n\n2.1 consistency metric衡量 temporal consistency 使用的是 ==3D pose acceleration error==\n分析之前的 methods 类似 HMMR 和 VIBE\n发现 似乎有 consistency 和 per-frame accuracy 之间的 trade-off\n3. TCMR3.1 temporal encoding每个 frame 过 resnet 生成 features，2048-dim\n利用 3 个 GRU units:\n\n$\\mathcal{G}{all}中间点是\\lfloor \\frac{T}{2} \\rfloor总共长度是，生成\\mathcal{g}‘{all}$\n\n$\\mathcal{G}{past}起始是0终止是 \\lfloor \\frac{T}{2} \\rfloor -1生成\\mathcal{g}’{past}$\n\n$\\mathcal{G}{future}起始是T-1终止是\\lfloor \\frac{T}{2} \\rfloor +1生成\\mathcal{g}’{future}$\n\n\n\npast 和 future 都是要 predict current frame 的\n\n\n==一个问题是: 既然 temporal 已经被 past 和 future 包含了，为什么还需要 bi-directional GRU?== \n文章这里的解释是 不用 residual connection 来使模型 strongly depend on current frame\n\n\n注意，这里 GRU predict 出来的都是 temporal features 了，之后就给 SMPL regressor 了\n\n\n\n\n\n3.2 integration\n\n利用了类似 attention module 来 生成 attention weight , 最后产生 输入 SMPL regressor 里面的 \n\n训练的时候，分别训练 $\\mathcal{g}’{past}给和当前对比，对于\\mathcal{g}’{future}和g’_{int}$  同理\n\n\n测试的时候直接 \n\n3.3 lossquote, “L2 loss between predicted and groundtruth SMPL parameters and 2D/3D joint coordinates are used”\n4. implementation detailshuman are cropped from image, and then cropped region is resized to 224x224\n\n==这里的担忧是 resize 不就产生 deformation 了吗？更好的办法不应该是 缩小然后 padding 吗？==\ntodo: 查一下代码是怎么操作的!!!\n\n5. experiments发现如果移除了 residual connection, acceleration error 小了很多，而且离谱的是，在没有增加 PoseForecast 的时候，PA-MPJPE 反而下降了\n\n?????? 减少了 current frame residual connection, 我们应该看到 single frame accuracy 下降才对???\n\n\n\n\n这是这里的解释。。。\n\n\n这里对比的 SOTA 模型全部：\n\n\n\n\n\n6. Other这里还发现了 attention 里面，past and future weights are larger than current weight\n最后提到了一个 temporal smoothing 的 technique，应该是能接着降低 acceleration 的\n发现 FPS 从 15 到 30，acceleration error 也下降了一半\n","categories":["paper review"],"tags":["computer vision","3d human pose and shape","video"]},{"title":"荷兰式竞拍","url":"/2021/11/14/paper_review/Dutch%20Auction/","content":"[[https://youtu.be/1UK0KQV03Wc]]\n用于很多 IPO 项目，希望散户也来参与的拍卖\n总结起来就是：\n开始拍卖 10 个 coins, 公司定一个最高价，之后逐渐递减。\n竞拍者可以在一个价格说出自己想要买的数量，比如说：\n\nA 要 6块 买 6 个，commit 36 元\nB 要 5 块买 2 个，commit 10 元\nC 要 4 块买 2 个，commit 8 元\n\n有两种方式结束竞拍：\n\n价格触底但是 coins 还没卖完 （触底价会提前定好）\n东西卖完了\n\n规则是，最后卖完的 价格 就是每个人要付的价格，每个人想要买的数量不变，所以 如果最后 4 元结束，A 会得到 36/4=9 coins. \n之后结束的时候就是 first-come-first-serve 的逻辑\n为什么？很多 IPO 使用 dutch auction, 目的是减少公司和投行的损失.\n很多公司上市的时候，会由 投行 根据 roadshow 来定价，最后在上市的那天以这个价格卖出。但是出现很多在第一天开始交易的时候，价格就暴涨。这样 公司和投行就损失钱了，因为卖便宜了。\n于是出现了用 dutch auction 的办法，这样就能卖出一个比较合理的价格。\n思考：这是因为发行价固定而导致的后期股票价格上涨，其实从上往下或者更直接一些，从下往上报价然后（有炒作嫌疑），然后没人想买的时候（价格达到预定最高价的时候），剔除最低价的，然后从最高价往下排列分 shares。\n有说法是能更 民主，希望散户也能进来投资。这个也有一定道理。\n","tags":["DeFi"]},{"title":"PARE- Part Attention Regressor for 3D Human Body Estimation","url":"/2021/11/14/paper_review/PARE-%20Part%20Attention%20Regressor%20for%203D%20Human%20Body%20Estimation/","content":"主要解决 single image 3D human pose and shape estimation 里面的 occlusion 问题\n1. 问题SOTA single image method 经常是 ResNet 提取 features 之后直接 regress SMPL params. \n文章认为 这种提取出来的 global feature, 对于 pixel changes 非常的 sensitive，所以对于 occlusion performance bad\n1.1 Occlusion存在 self-occlusion, close-range interaction with other people occlusion, and occlusion due to objects\n2. Intro主要的方法是：\n分两个branch：\n\n3D body params branch，产生  dimensions 的能直接 regress SMPL params 的 features,  就是 embed dimensions\n2D joints segmentation mask, 产生类似 attention 的  dimensions 的， 是 number of joints, 对于每个 pixel 都产生 probability that it belongs to one joint.\n\n这里的 intuition 是，to be robust to occlusion (pixel changes), model should leverage pixel-aligned image features of visible parts to reason about the occluded parts. \n\n这里 ==pixel-aligned image features 我的理解是 pixel-wise features.== \n但是不够，==这里也希望说 occluded parts 不能影响到 visible parts, 然后还希望 visible parts help occluded parts.==\n所以需要 attention 来生成 joint features dependent only visible parts pixels (soft-attention mask)\n\n这里的训练方法是 2D joints segmentation mask 先分开 supervised training，之后再和 3D body params branch 一起训练\n\n这里也是实验得出的 效果最好的训练方式\n后面会提到，对比了三种训练方式:\n\n不单独训练 2D，直接一起训练 (效果最差)\n单独训练 2D 之后，weight freeze\n单独训练 2D 之后，合在一起训练 (Best)\n\n\n这里 第三种的意思 是，希望 attention 能学习到 用 visible 来 预测 occluded，**(希望不只是 joints segmentation mask 了)**\n之后 combine 在一起之后直接 regress SMPL parameters\n\n看 Methods 里面的 图片\n\n3. related works3.1 implicit occlusion handling (data augmentation)\n之前的办法就是 data augmentation, cropping frame, overlaying patches as occluding body parts\n\nCheng et al. apply augmenta\u0002tions to heatmaps that contain richer semantic information and hence occlusions can be simulated in a more intelligent way\n\n就是说在 heatmaps 上面做 augmentation，比起直接挡掉更能模拟真实的 occlusion\n\n\n\n作者认为虽然有帮助，但是 不够 平常的 occlusions 那样 complex，而且作者认为模型架构上需要改变\n总之对于 implicit occlusion 就是看不起\n3.2 explicit occlusion handlingCheng et al. [9] avoid including occluded joints when computing losses during training. Such visi\u0002bility information is obtained by approximating the human body as a set of cylinders, which is not realistic and only handles self occlusion.\n\n把人想成 set of cylinders, 然后把 occluded joints 部分的 loss 在 training 的时候删除\n作者认为 这种 approximation 不好，而且只能处理 self occlusion\n\nWang et al. [56] learn to predict oc\u0002clusion labels to zero out occluded keypoints before apply\u0002ing temporal convolution over a sequence of 2D keypoints.\n\n这里是想 预测 occlusion label for keypoints，我理解是想法和上面类似，把 occluded 的部分的 loss 删除\n\nZhang et al. [61] leverage saliency masks as visibility information to gain robustness to scene/object occlusions. Human meshes are parameterized by UV maps where each pixel stores the 3D location of a vertex, and occlusions are cast as an image-inpainting problem.\n\n用 saliency map, 把人体弄成 UV maps，然后 occlusion 被看成是 image inpainting problem，我理解是 直接涂掉是 一样的\n作者认为 in-the-wild data 可能没有很好的 saliency maps, UV-coordinates 能导致 mesh artifacts.\n\n4. occlusion sensitivity analysis就是检测 算法对于 occlusion 的 sensitivity 程度的\nTo extract features from the input image I, current di\u0002rect regression approaches [24, 29] use a ResNet-50 [17] backbone and take the features after global average pool\u0002ing (GAP), followed by an MLP that regresses and refifines the parameters iteratively.\n\n当前的方法就是 resnet50 提取 image feature, 之后 global average pooling,  最后 MLP 生成 SMPL params\n\nZeiler et al. [60] who systematically cover different portions of the image with a gray square to analyze how feature maps and classififier output changes. In contrast, we slide a gray occlusion patch over the image and regress body poses using SPIN [29]. Instead of com\u0002puting a classifification score as in [60], we measure the per\u0002 joint Euclidean distance between ground truth and predicted joints.\n\n这里的 difference 感觉比较 trivial，我的理解是这里的 sliding 更有更全一些的 output，就之前是 different portios, 这里是 every pixel\n\nWe create an error heatmap, in which each pixel indi\u0002cates how much error the model creates for joint j when the occluder is centered on this pixel.\n\n这里就会产生在 遮挡 square 的中心点在 某个 pixel 上时，预测出来的 joint 和 GT joint 的 error 是多少，然后这个 error 就导致了heatmap 上 这个 pixel 的颜色。\n\n\n\n\n这里是跑 model SPIN 的结果，作者从中得出三个结论：\n\nerror集中在人的pixel上面，说明 SPIN working\n\n原来能看见的 joints 现在被挡住了产生的 error 更大\n\n==本来就挡住的 joints，能从其他的 joints 中 infer 出来  可以从对比 left/right ankles(occluded) 的 error 当把 thigh region occluded 的时候，发现 error 较大，说明有利用到 thigh 方面的  features==\n\n\n\n5. Methods5.1 SMPL 简单介绍SMPL [33] represents the body pose and shape by , which consists of the pose  and shape  parameters. Here we use the gender-neutral shape model as in previous work [24, 29]. Given these parameters, the SMPL model is a differentiable function that outputs a posed 3D mesh . The 3D joint locations  are computed with a pretrained linear regressor .\n\n就是说 3D poses 里面包含了 24 个 joints (24 x 3 = 72), 然后 shape params 是 10-dims 的，最后生成的 mesh 有 6890 个 keypoints on surface. \n这里的 W 就是把 SMPL 的 M 最后提取出 24 个 joints 的，用来和 GT 3D joint 对比来做 loss\n\n5.2 Methods\n\n先是 通过 CNN backbone (ResNet50) 来提取出来 pixel-aligned volumetric feature\n之后通过 two branches:\n\n2D part branch 来得到 “类似” joint segmentation mask 的 features, , dimension = \n3D body branch 来直接生成给后面 SMPL regressor 用的 features, , dimension = \n\n之后我们整合起来得到 \n\n 理解成 soft attention mask, 中间的乘积叫做 hadamard product, 其实就是 elementwise product, 所以是 attention\n然后  就是 num_joints  embed_dims 的结构，类似于 joint features\n\nThis attention oper\u0002ation suggests that if a particular pixel has a higher attention weight, its corresponding feature contributes more to the final representation \n\n就类似于去 pixel-aligned 的 features 里面找能相关的 pixels，开始的时候 2D joint map 就是 segmentation mask, 后面就变得灵活了\n\nIn the case of occlusion, however, if we predict part segmentation as  , the feature for the joint  can aggregate per-pixel features only belonging to that particular body part.\n\n这就是需要后面  变得灵活的原因，不然的话 occluded joint feature 就都是 0 了，这里灵活之后就希望能够学习用 visible joints 来 infer missing joints\n\n5.3 losses\n\n\n 就是 2D segmentation mask 的 loss，作者的意思是 前期  nonzero, 后期给 调成 0 来使学习 attention\n\n6. implementation detailsTo increase robustness to occlusion, we use common oc\u0002clusion augmentation techniques; i.e. synthetic occlusion(SynthOcc) [45] and random crop (RandCrop)\n\n还是用了 occlusion data augmentation 的\n\n\n\n\n这就是 ablation study 的结果，重点关注，parts, unsup, and parts/unsup，就代表了之前说的 三种  的方法\n\n\n\n\n这是 occlusion data augmentation, 其实就是 inpaint objects over the person\n\nChallenging Cases\n\n\n\n\n\n","categories":["paper review"],"tags":["computer vision","3d human pose and shape","occlusion"]},{"title":"SimPoE Simulated Character Control for 3d human pose estimation","url":"/2021/11/14/paper_review/SimPoE%20Simulated%20Character%20Control%20for%203d%20human%20pose%20estimation/","content":"todo:\n\n了解为什么没有 flaws，然后具体是怎么解决的\n了解一下 root torque 是怎么分成 15 steps 来 apply 的，看看之前的 paper\naction generation unit 不是 RL 吗，但是这里好像是 MLP 直接 infer action？？？\n增加 metric 的 formula 啥的\n\n1. intro1.1 problem:kinematics 已经很多研究了，但是 dynamics 还没怎么研究.\n\nkinematic: 主要集中在 geometric relationships of 3D poses and 2D images. 但是只考虑 positions 的，文章指出因为没考虑 physical forces (dynamics), 所以经常出现物理条件下不可能的情况：\n\n穿模\n在视频中预测的 3d 模型抖动\n脚在地面上滑行\n\n\ndynamics: 也关注了 physical forces 等类似 friction，joint torque 啥的\n\n\n1.2 加入了 dynamics 的 related works:大部分都是 先 kinematic motion estimation, 之后再用 physics-based trajectory optimization (two stages)\n但是存在问题：\n\ntrajectory optimization highly complex，而且不是 train 出来的，只是 post-processing 的一个 step，所以 test-time 有很长 latency\n用 simple，differentiable physics model 来 optimization，但是更准确的 model 都是 un-differentiable 的，所以这种办法可能有 high optimization error.\n因为是 post-processing, 不能 end2end learning\n\n1.3 用 RL 来模拟 character control 的\n有 manually driven awards 的\n有 GAIL based method RL 不用 reward engineering 的\n\n来达到 long-term behavior，有的 work 用了 hierarchical RL 来 control characters\n最近的 work 来学习 user-controllable policies from motion capture data. 但是，这种学习智能 reproduce training motions, 对于没见过的就不行了\n\n????? 了解一下具体是怎么有 flaws，然后这个是怎么解决的\n\n2. method![image-20211001201237465](SimPoE Simulated Character Control for 3d human pose estimation.assets/image-20211001201237465.png)\n2.1 在 simulation 里面创建一个人的model用 SMPL model 的算法 类似 VIBE 来产生 skinned mesh human model，这些模型提供了：\n\nskeleton of  bones\nmesh of  vertices\nskinning weight matrix , 每个 i-j element 都代表了 influence of  th-bone’s transformation on  th-vertex’s position\n接着把每个 vertex 分配到 bone，从  里面看哪个 bone 对 这个 vertex 的影响最大就分配给 哪个bone， 计算得到  \n用 convex hull 来计算 bone 的 geometry，接着假设 constant density of bone，mass 就由 geometry 决定\n\n\n\n2.2 simulate character control2.2.1 用 PPO 算法（2017）来做 RL2.2.2 定义 states：$s_t = (q_t, \\dot{q_t}, \\tilde{q}{t+1}, \\check{x}{t+1}, c_{t+1})$:\n 是 current pose\n 是 joint velocities\n 是 estimated kinetic pose\n 是 2D keypoints\n 是 keypoint confidence\n\n2.2.3 定义 actions:\n一般的 action 设计是计算 每个 non-root joint 的 从一帧到下一帧的 torque. \n\n一个 sampling difference 问题：这里存在 video 是 30Hz 的，但是 physics simulator 是 450Hz 的，所以我们每帧之间的 action 需要分配成 15 个 simulation steps. 这里我们使用 proportional-derivative(PD) controllers 来分解这 15 个 steps，生成 15 个action来得到最终效果，PD controller 类似于 damped spring. \n\nPD 的 定义： \n\n 就是每一个 step 施加的 torque\n 是 target joint angles of PD controllers\n 分别代表 在这个 step 开始的时候的 joint angles 和 joint angle velocity\n 分别代表 stiffness 和 dampness of spring, 是 hyperparam，需要 manual 调整\n\n\n我们之后 介绍 meta-PD controller, 为了弥补上面需要 手动调整  的问题，我们引入 \n\nprior works 介绍了如果给 root joint 加 torque 提升模型 robustness，所以 action 再 predict residual forces and torque  给 root\n\n？？？？？？这里文章没说 root torque 是怎么分成 15 steps 的，如果没有用 meta-PD 的话，就可能 apply same torque 15 times 来训练也行，可能隐藏在 prior works 里面了，参考 “residual force control for agile human behavior immitation and extended motion synthesis. 2020”\n\n\n\n\n\n2.2.4 定义 rewards：\n\n\n 是 pose reward, 计算 difference of local joint orientations  and ground truth \n\n\n\n\n is number of joints,  is relative rotation between two rotations\n\n\n is velocity reward\n is 3d world joint position reward\n\n\n\n\n is 2d projection of joints to match 2d joints\n\n\n这里选择 multiplication of sub-awards 由 prior works “A scalable approach to control diverse behaviors for physically simulated characters” 显示，保证说 每个reward都不会被 overlooked.\n\n\n2.3 kinematic aware policy\n这里 RL 的 explore 是定义成 normal distribution 来选择 action 的，这里定义一个 mean 和 variance，mean 在最后应该是 optimal action\n\n\n\n这里  是 kinematic refinement unit, 这里的  就是 kinematic pose after n iterations of refinement. \n 是 control generation unit, 通过当前 pose 和 velocity 和 下一帧的 pose，能得到这些值\n这里没有直接 regress 到 ，因为 author 说这样 learning easier\n2.3.1 kinematic refinement unit \n\n这里定义 MLP ，  是 gradient of reprojection loss , \n\ninspired by prior work “Human body model fitted by learned gradient descent”, 这里不是想 minimize loss，只是想用这个 as informative kinematic feature to learn a pose update 是最后的 pose stable 且 accurate\n\nreprojection loss  定义：\n\n\n大写 X 是 3d 的，小写是 2d 的，这里就是做了一个 projection，并且乘上了 2d joints uncertainty, 来 account for keypoint uncertainty.\n\n z converted to character’s root coordinate to be invariant of character’s orientation\n\n\n文章这里说 因为 dynamic 那边用了 kinematic 的预测，然后在 RL 里面 train 的时候。就类似于 jointly training 了\n\n2.3.2 feature extraction layer这里在 直接把 current pose, pose velocity, 和 next pose 传给 action generation unit 之前，先传给 feature extraction layer, 再经过 normalization layer 之后，再传给 最后的一个 MLP 来生成 action\n\n?????? 这里不应该是 RL 吗，但是这里看到的好像是 MLP 直接 infer？？？todo\n\n2.3.3 Meta-PD control主要关注  和  的 ratio，large ratios lead to unstable and jittery motions, small ratios 可能就太 smooth 然后 lag behind ground truth.\n我们使用两个 initial params  和 ，然后对于每个 15 steps，我们生成参数  and  来得到每一个 step 的 新的  和 \n\n\n\n\n3. Experiments3.1 Metrics检验 shape accuracy 的\n3.1.1 MPJPE (mean per joint position error)\ntodo: …. add formula\n\n3.1.2 PA-MPJPE (procrustes-aligned mean per joint position error)\ntodo; … add formula\n\n检验物理稳定性的\n3.1.3 Accel (difference in accleration between the predicted 3d joint and GT)\nTodo: …. add formula\n\n3.1.4 FS (foot sliding)对比 body mesh vertices that contact the ground on 连续 2 帧，compute average displacement within frames.\n\ntodo:…. add formula\n\n3.1.5 GP (ground penetration)compute average distance to the ground for mesh vertices below the ground.\n\nTodo: …. add formula\n\n3.2 datasets这里用了 human3.6M (S1, S5, S6, S7, S8) 做 training，(S9, S11) 做 testing\n还有 in-house human motion dataset，其中包括了 detailed finger motion.\n3.3 具体的测试3.3.1 simulation character createusing SMPL model for Human3.6M\nFor in-house dataset, use non-rigid ICP and linear blend skinning 来 reconstruct skinned human mesh model.\n\nTodo: 1. non-rigit ICP: optimal step nonrigit icp algo for surface registration; 2. linear blend skinning: skinning with dual quaternions\n\n3.3.2 initializationfor Human3.6M, VIBE 直接 predict pose \nFor in-house dataset, 自己创建了一个 kinematic pose estimator\n3.3.3 Other detailsexperiments 中，之前的 physics-based method 产生很大的 Accel 因为 character 经常 falling when losing balance. (但是其实人没有倒…) \n4. limitations这里因为用了 3d scene simulation 来 enforce contact constraint during motion estimation. 所以对于 in-the-wild dataset 就不能用。\n5. Idea1. simulationReplace simulation with discriminator to support motions in the wild\n2.","categories":["paper review"],"tags":["computer vision","3d human pose and shape","Reinforcement Learning"]},{"title":"end to end human pose and mesh reconstruction with transformers","url":"/2021/11/14/paper_review/end%20to%20end%20human%20pose%20and%20mesh%20reconstruction%20with%20transformers/","content":"1. Intro1.1 parametric model:利用 ==提前定义好的 parametric model== 像是 SMPL，最新的是 STAR 来直接 regress params\n\n有 strong prior about human body, 所以对于 pixels environment 比较 robust\n\n受到 定义的 parametric model 的限制，因为是从 limited examples 里面 learned 出来的，所以作者认为不能很好 generalize\n\n\n1.2 直接 regress vertex:利用 ==3D mesh, volumetric space, 或者 occupancy field 来表示3D人体==\n第二种常见的方法就是 GCNN to model neighbor vertex-vertex interactions，或者 1D heatmap to regress vertex coordinates. 但是作者认为这种方法不能 capture non-local vertex interaction\n而且现对于 SMPL 来说就不是 很 robust 了\n1.3 inverse kinematic technologyIn computer graphics and robotics, inverse kinematics techniques[2] have been developed to estimate the internal joint positions of an articulated figure given the position of an end effector such as a hand tip.\n\ninverse kinematic techniques 我的理解是 机器手，给定机器手的顶端的位置，就能直接 estimate 每个 joint 的 位置，\n就说明各个 joints 之间存在 strong interaction\n\n2. related works2.1 loss 方面Since it is challenging to regress the pose and shape coefficients directly from an input image, recent works further propose to leverage various human body priors such as human skeletons [21, 34] or segmentation maps [29], and explore different optimization strategies[19, 17, 46, 12] and temporal information [18] to improve reconstruction.\n\n就是说 loss 里面结合 temporal，skeleton, segmentation maps 之类的方法来 improve reconstruction. \n\n2.2 non-prior representationresearchers have also proposed approaches todirectly regress 3D human body shape from an input image. For example, researchers have explored to represent human body using a 3D mesh [20, 7], a volumetric space [47], or an occupancy field [41, 42]. \n\n不用 prior models，直接 regress 的话 用来表示 body shape 的有：\n\n3D mesh; \nvolumetric space; \noccupancy filed\n\n\n3. methods先是有 CNN backbone 得到 2048-dim vector representation. (CNN 是 pretrained on ImageNet classification task), 就生成了下面的 左边的 粉红色的框\nTransformer 是需要queries的(类似于 posititonal encoding), 对于每一个要预测的 3d点 (joint or vertex), 把 一个3d query 和 上面的 feature 合并，然后得到 num_queries x 2051 的 matrix 给 Transformer 做 input.\nTransformer 因为 output size 不变，所以这里用 cascade of transformers, 两个之间使用 dimension reduction 的方法, 来最终得到 *3D outputs\n\n我的理解是 这个reduction可以是 MLP来做，或者更简单就直接是 pooling\n\ninput to transformer 是 joint 和 mesh vertex 的 query，这里用了一个 template human mesh model 来 preserve positional information of each query. \n\n我的理解是这种 mesh template 可变参数更多，对比 SMPL 更 general\n\n每个点都是  3D coordinate 作为 positional encoding, 这里直接 和 image feature concat 在一起，所以每个 query 都是 (img_feat) 2048 + 3 = 2051 features，3 就是 positional encoding of query\n就得到了 joint queries , vertices queries , \n\n这里 n 就是 number of joints, m 就是 number of vertices\n\n\n\n\n\n3.1 MVM (Mask Vertex Modeling)Masked Language Modeling (MLM) to learn the linguistic properties of a training corpus. However, MLM aims to recover the inputs, which cannot be directly applied to our regression task.\n\n在 NLP 里面用的是 MLM 的方法来做训练的，是去预测别的 query 下的词，作者认为这里不能直接用于 regression 3D coordinates\n\n所以提出了 MVM\nWe mask some percentages of the input queries at random. Different from recovering the masked inputs like MLM [8], we instead ask the transformer to regress all the joints and vertices.\n\nmask some input queries at random, 但是直接要 transformer regress all joints and vertices\n这里的 mask 方法没讲清楚是怎么mask 的，我的理解是不给 CNN features, 但还是给 positional encoding?????\n\n3.2 loss: 3d vertex 之间的 L1 loss\n: 3d joint 之间的 L1 loss\n: 存在 regression matrix G, 能从 vertex 里面计算出 joint 坐标，这里就是L1 loss of GT joint 和 用 G 算出来的 joint\n\n能从 mesh vertex 中提取 joint, 就是希望 mesh 能和 joint 对应上\n这里的思考是 为什么这里 ==不是和 predict 出来的 joint 对比，而是和 ground truth 的 joint 对比????????==\n\n: L1 loss of GT 2d joint point 和 投影出来的 2d joint point (这里在 transformer output 上加了一层 linear layer 能自动学习 camera params)\n最后是 \n4. implementation details4.1 coarse mesh learningour transformer processes a coarse mesh: (1) We use a coarse template mesh (431 vertices) for positional encoding, and transformer outputs a coarse mesh; (2) We use learnable Multi-Layer Perceptrons (MLPs) to upsample the coarse mesh to the original mesh (6890 vertices for SMPL human mesh topology); (3) The transformer and MLPs are trained end-to-end; Please note that the coarse mesh is obtained by sub-sampling twice to 431 vertices with a sampling algorithm [37]. As discussed in the literature [20], the implementation of learning a coarse mesh followed by upsampling is helpful to reduce computation. It also helps avoid redundancy in original mesh (due to spatial locality of vertices), which makes training more efficient.\n\nmesh template 只有 431 vertices, 最后生成的 3D mesh 有 6890 vertices, 这里是 subsample 下来之后，先预测 coarse vertices，之后用 MLP 来upsample到 6890 vertices. 这里用了 [37] 的sampling methods twice 才得到 431 vertices 的 coarse mesh. \n而且 [20] 还证明了 learning coarse mesh 再 upsampling 能 reduce computation, 而且 avoid redundancy in original mesh???? \n(我的理解是，很多点之间位置其实存在强关系，类似于中点之类的，所以不需要全部包括进去)\n\n5. experiments5.1 datasets:Human3.6M: 2d + 3d annotations, 但是 3d mesh 有 license 不能用，转而使用 pseudo 3D meshes provided in papers: \n\npose2mesh, ECCV, 2020    \nI2Lmashnet, ECCV, 2020\n\n3DPW: outdoor 2d + 3d annotations, training images 22K, testing 35K.\nUP-3D: outdoor image data, 3d annotations 是 model fitting 创造的, 7K training images.\nMuCo-3DHP: synthesized data from MPI-INF-3DHP dataset, 有很多 real-world background images, 总共 200K training images.\nCOCO: 3d data 是 papers 生成的:\n\nlearning to reconstruct human body pose and shape shape via model-fitting in the loop, ICCV, 2019\n这里使用了 Simplify-X 来 fit 的: expressive body capture: 3d hands, face, and body from a single image, CVPR, 2019\n\n\n\nMPII: outdoor image with 2d poses, 14K training images\nFreiHAND: 3d hand dataset, 130K training, 4K testing.\n5.2 Metrics:MPJPE: mean-per-joint-positioning-error ==(用于3d pose)==: euclidean distance between GT joint 和 predicted joints\nPA-MPJPE: ==(一般用于 reconstruction error)==, 用 ==Procrustes Analysis(PA)== 来做 3d alignment，然后再算 MPJPE, 因为和 scale 和 rigid pose (rigid transformation)没关系\nMPVE: mean-per-vertex-error, euclidean distance between GT vertex 和 predicted vertex\n6. Result analysis:\n\n\n这里提取 attention weight 来 analyze vertices interaction, 颜色越亮代表 weight 越大\n\nAt the first row, the subject is severely occluded and the right body parts are invisible. As we predict the location of right wrist, METRO attends to relevant non-local vertices, especially those on the head and left hand. At the bottom row, the subject is heavily bended. For the head position prediction, METRO attends to the feet and hands (6th column at the bottom row).\n\n\nestimate an overall self-attention map. It is the average attention weight of all attention heads at the last transformer layer.\n\n就是估计 self-attention map\n\nWe visualize the interactions among 14 body joints and 431 mesh vertices in Figure 4. Each pixel shows the intensity of self-attention, where darker color indicates stronger attention. Note that the first 14 columns are the body joints, and the rest of them represent the mesh vertices. We observe that METRO pays strong attentions to the vertices on the lower arms and the lower legs. This is consistent with the inverse kinematics literature [2] where the interior joints of a linked figure can be estimated from the position of an end effector.\n\n这个 visualization 不错，前14个是 joint, 后面都是 vertices，颜色越深代表weight越大。\n发现 lower 的四肢 的 weights 一般都很高，这个和 之前机械手 的 那个结论吻合\n\n6.1 transfer to other tasks这个 transformer 的 structure 可以很容易 transfer 到其他 tasks 上面去，像是 3D hand in-the-wild reconstruction,\n其实只要替换一下 template mesh, 然后重新训练就 ok\n\n能 easily transfer to other tasks. 而且 在 3D hand reconstruction 上面也是 SOTA\n\n","categories":["paper review"],"tags":["computer vision","3d human pose and shape","transformers"]},{"title":"liquidity pool","url":"/2021/11/14/paper_review/liquidity%20pool/","content":"[[https://youtu.be/cizLhxSKrAc]]\n我们先从传统 股票 trading 开始，传统的股票trading依赖 order book model，就是 buyer 定价，seller 也定价，只要价格上有 match，就成交。\n但是这种方式存在问题：\n\n如果价格一直没有match，就一直不能成交\n如果 buyer 或者 seller 虽然定价了，但其实没有这么多的 钱 或者 股票，不能成交\n\n这时候是有一群 market maker to facilitate trading, always willing to buy or sell certain an asset. ?????? \n\nthey provide liquidity, so that users can always trade without another counterparty to show up\n\nmarket makers unsuitable for DeFiOrder Book Model relies heavily on market makers, and order book model is unsuitable for DeFi\nmarket makers track the current price, and constantly change the price, 这在 DeFi 里面就会产生很多的 blocks，因为 ETH 的 processing power limited, 而且每次执行 ETH 的 DeFi 都要提供 gas fee, 所以 market makers 每次 update order 都要付 gas, cost is too high.\n这里还提到了 second layer scaling.\n\n\n\n这里的意思是 market makers 自己可能也有 liquidity issues\n而且 single user 想要 trade 的话，要把东西 move in and out of the 2nd layer, so 2 extra steps, inefficient\n\nhow Liquidity Pool works比如说 uniswap 里面想要 交换 DAI 和 ETH. \n想要交换的双方提供 tokens，DeFi 对于每一种 pair of tokens 都创建 new market.\n然后会有 liquidity provider (LP) that sets the initial price of exchange, DeFi 这里用 incentive 来给 LP 希望能 set equal values of both tokens.\n\n如果 initial price 和 market value diverges, 就会创造 **arbitrage opportunity (套利空间)**，会导致 LP lose capital \n\n之后还可以有 新的 LP 进来把钱放进 liquidity pool\n每个 LP 把钱放进 liquidity pool 之后，会得到 proportional number of LP tokens, 之后每次交换都会 收取 0.3% 的 fee. 这个 fee 就会均匀的分给 LP token holders based on the number of LP tokens.\n\n如果 LP 想把钱拿回来，就把 LP tokens 给销毁就能拿回 钱 和 incentives 了\n\nAMM (automatic market maker) 是 deterministic algorithm 能自动 price adjustment. 不同的 平台类似 uniswap 和 别的就 区别在这个算法上面\nUniswap 的 constant product market maker 算法\n\n我的理解是 本来 DAI 价格等于 0.5 ETH, 然后 x = 1 个 DAI, y = 1 个 ETH, k = 0.5. 之后如果 有人想要拿更多的 DAI 出来换 ETH，x 上升，y 下降，导致 ETH 的价格变高\n\n 这种算法的好处是 can always provide liquidity, no matter how larger a trade is ????\n\n简单来说就是, ratio of the token in the pool dictates the price (注意这里不是 trade 的 tokens 数量)\n所以 bigger pool 导致 lesser price impact 导致 lower slippage \nother algos\nBalancer 可以支持 up to 8 tokens in a market\nCurve 认为 UniSwap 对于 similar price tokens 之间交换有问题，改进后的 similar price tokens 有 lower exchange fee, and lower slippage\n\n","tags":["DeFi"]},{"title":"Real-time Small Objects Falling from High Detection","url":"/2021/08/23/projects/gaokongpaowu/","content":"1. Project Introduction1.1 Background:Nowadays, thanks to the fast development, tall buildings with more than 20 floors are everywhere in China, and the ratio of tall buildings is increasing. With the convenience of accomodating more people in the city, tall buildings also pose a threat to the safety of others. \nWhen people live in the high floor, some casual, and barely innocent behaviors become deadly, for example, throwing things out of the window. When the object falling from the high, its great potential energy directs to such strong momentum that it can cause death to people being hit. Findings said: a 30g egg falling from the 4th floor can cause a bump in the head; falling from 8th floor can hurt the scalp; falling from 18th floor can hit through the skill; falling from 25th floor can cause death immediately. \nAlthough there have been laws punishing such behavior, there still exist some people throwing things like garbage, and watter bottle, etc. Nowadays, several cameras will be installed facing upward and record the sides of the buildings 24/7. If there is one incident of objects falling from the high, the police will look through the recordings to determine the criminal. \nHowever, these is usually one camera for each side of the building. Object like an egg in the image will have less than 10 pixels in size. It is difficult to stare at the recordings for hours and detect such small objects falling from one window. So replacing the original camera with a new device that can quickly detect such incident and alert the police beomes an urgent need.\n\n1.2 Objectives:Developed a system that integrates a camera recording upward 24/7. a real-time algorithm that detects objects falling, generates evidence with object’s falling trajectory, and automatically alert the police. \n1.3 Difficulties:Popular neural network based CV method cannot work because of these reasons:\n\nObjects look very small (&lt; 10 pixels) in the image. \nOverall system should not be expensive (hardware does have much processing power like GPU) because objects falling from the high is rare.\n\nThere is no size information about an object. object like a bird or a bug can look like a large object far from the camera. \nVarious weather conditions like strong wind that cause the camera to shake, rainy and snowy days that greatly increase the false alarm percentage of the system.\nObjects have similar color as the building. In its falling video, the object may ‘“vanish”‘ for a few frames, making its tracking difficult. What is more, the window on the side of the building reflects the sky, and bird flying at centain angle may look like a falling object on the window.\n2. Software and Hardware Environment2.1 Hardware:Windows10 64bits, 8GB RAM, Intel Pentium CPU G4560@3.5GHz\n2.2 Software:Python3.7：{psutil, websockets, tqdm, scikit-learn, pyinstaller, loguru, numpy, opencv-python, opencv-contrib-python}\n3. Overall System Designst=&gt;start: Startop1=&gt;operation: Read a frame from cameras continuouslyop2=&gt;operation: Process a frame, recognize moving objectscond1=&gt;condition: Has moving object?op3=&gt;operation: Multi-objects tracking generates trajectorycond2=&gt;condition: Is a valid falling trajectory?op4=&gt;operation: Generates evidence with trajectory drawnop5=&gt;operation: Save evidence locally, upload alert image and evidence video to FTP server, and alert the remote server.st-&gt;op1-&gt;op2-&gt;cond1cond1(yes)-&gt;op3cond1(no)-&gt;op1op3-&gt;cond2cond2(yes)-&gt;op4cond2(no)-&gt;op1op4-&gt;op5-&gt;op1\n\n\n\n4. System Functions Design:4.1 Read video:4.1.1 Supports reading video from camera in real-time (around 25 fps)\n4.1.2 Supports setting skip_frame parameter(actual processing speed becomes 25/skip_frame fps), in case the algorithm is not fast enough\n4.2 Process frame to detect moving objects:4.2.1 Image preprocessing: use Gaussian Blur to denoise\n4.2.2 Applies frame difference on consecutive frames, and use adaptive threshold to filter the moving area. \n4.2.3 Compare the ratio of total moving area to the entire area. If this ratio exceeds a threshold, decides this as an invalid frame. \n\n\nThis helps to decrease the false alarm percentage during rainy and snowy days.\n\nUse two-buffer that both store several frames up to the current frame, compare if the image has become stable and ready to detect again. \n\n\n\n4.2.4 Performs image dilation on the resulting moving area image to merge near-by moving area. Filter moving area by its size and width-height ratio.\n4.2.5 Inframe Denoise: Extracts centers of these moving area. Keep only one center with the highest y-value in a 40x40 area.\n4.2.6 Between-Frames Denoise: Relates the Idea from the background subtraction. Compare moving area in the recent frames. if there exist repeated moving areas across several recent frames, consider them as flicker noise.\n4.3 Track moving objects across frames:4.3.1 Idea is to compare the current moving objects with predictions from several previous frame’s Kalman Filters and selects the most possible one. Each kalman filter corresponds to one falling object.\n4.3.2 Define a min_dist parameter that will decide if a moving point is part of the previous trajectories. If a new moving point belongs to one trajectory, updates that corresponding kalman filter. Otherwise, creates a new Kalman Filter for this moving point. Decide if a new moving point belongs to one trajectory following these rules:\n\n\nCompare the likelihood of a new moving point with predictions from all Kalman Filters. \n\nOut of all trajectories, if many trajectories have similar likelihood, the new point will belong to the longest trajectory.\n\n\n\n4.3.3 Decide if current trajectories exist object falling trajectory:\n​    4.3.3.1 Number of detected frames must exceed the min_falling_frames threshold.\n​    4.3.3.2 The y-value difference between the highest point and the lowest point must exceed the min_yfall_total threshold.\n​    4.3.3.3 The falling speed of the trajectory must satisfy some thresholds.\n​    4.3.3.4 The x-direction moving speed must not increase during the entire trajectory.\n​    4.3.3.5 Smoothen the trajectory and decide if there exist too many segments that is less than 45 deg (unlikely to see in a normal falling trajectory).\n​    \n4.4 Generate alarm image and evidence video and upload4.4.1 A seperate thread keeps receiving frame and is_falling signal from the processor. Once a is_falling signal is detected, starts a new thread that draws bounding box around the falling object, and draws the trajectory of the falling object.\n4.4.2  Save the alarm image and evidence video locally and inserts a record into local database. \n4.4.3 An upload thread will continuously read the database if one new record is inserted, and then upload the falling information to the remote FTP server.\n4.4.4 Send alarm including the time and location of the detection, and http download link of evidence to the remote server using websocket. \n5. Client-side Control UI (only Chinese version):5.1 Starting Panel\n\n\n\n5.2 Adding a new camera and configure\n\n\n\n5.3 Drawing IoT for detection\n\n\n\n5.4 Finish adding the device\n\n\n\n\n6. Example Evidence Video\n\n\n","categories":["project"],"tags":["computer vision","python","tracking"]}]